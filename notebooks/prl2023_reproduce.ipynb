{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4804b30-0e7d-45a6-8701-5cbe72973532",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pysindy as ps\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import pickle\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "# Set environment variable for NumPy (optional, for performance as in code)\n",
    "#os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Parameters\n",
    "n = 20          # Number of nodes (small for toy model)\n",
    "m = 2           # Dimensions per node (Rulkov: u, v)\n",
    "time = 500      # Time steps for data\n",
    "transient = 10000  # Transient steps to discard\n",
    "beta = 4.1      # Rulkov parameter for chaotic dynamics\n",
    "mu = 0.001      # Rulkov parameter\n",
    "sigma = 0.001   # Rulkov parameter\n",
    "C = 0.1         # Coupling strength (weak)\n",
    "gamma = 0.001   # Noise amplitude\n",
    "eta = 0.2       # Weight variation for network edges\n",
    "\n",
    "# Storage dictionary for results\n",
    "storage = {\n",
    "    \"adj_matrix\": [],\n",
    "    \"laplacian_matrix\": [],\n",
    "    \"delta\": [],\n",
    "    \"data\": [],\n",
    "    \"dist_matrix\": [],\n",
    "    \"Fx\": [],\n",
    "    \"Y_hub\": [],\n",
    "    \"Y\": [],\n",
    "    \"predicted_laplacian\": []\n",
    "}\n",
    "\n",
    "# Output file\n",
    "filename = f\"../data/statistics_data/statistics_gamma_{gamma}.pkl\"\n",
    "f = open(filename, 'wb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb5d019-3bec-4dd0-865a-873a9a0756cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjacency matrix shape: (20, 20)\n",
      "Laplacian matrix shape: (20, 20)\n",
      "Max in-degree (delta): 3.026245420104368\n"
     ]
    }
   ],
   "source": [
    "def network_generate(n, eta):\n",
    "    # Generate scale-free directed graph\n",
    "    G = nx.scale_free_graph(n, alpha=0.2, beta=0.3, gamma=0.5)\n",
    "    # Remove self-loops\n",
    "    G.remove_edges_from(nx.selfloop_edges(G))\n",
    "    # Remove multi-edges (keep first edge per pair)\n",
    "    edges_to_remove = []\n",
    "    seen = set()\n",
    "    for edge in G.edges:\n",
    "        if (edge[0], edge[1]) in seen:\n",
    "            edges_to_remove.append(edge)\n",
    "        else:\n",
    "            seen.add((edge[0], edge[1]))\n",
    "    G.remove_edges_from(edges_to_remove)\n",
    "    # Assign random weights\n",
    "    for edge in G.edges(data=True):\n",
    "        edge[2][\"weight\"] = np.random.uniform(1.0 - eta, 1.0 + eta)\n",
    "    # Adjacency matrix (transposed as in paper: w_ij from j to i)\n",
    "    A = nx.adjacency_matrix(G).todense().T\n",
    "    # In-degrees (weighted)\n",
    "    k_in = np.zeros(n)\n",
    "    for node in range(n):\n",
    "        if G.in_degree(node) > 0:\n",
    "            k_in[node] = sum(data[\"weight\"] for _, _, data in G.in_edges(node, data=True))\n",
    "    # Laplacian: L = diag(k_in) - A\n",
    "    L = np.diag(k_in) - A\n",
    "    delta = np.max(k_in)\n",
    "    return G, A, k_in, L, delta\n",
    "\n",
    "# Generate one network realization\n",
    "G, A, k_in, L, delta = network_generate(n, eta)\n",
    "storage[\"adj_matrix\"].append(A)\n",
    "storage[\"laplacian_matrix\"].append(L)\n",
    "storage[\"delta\"].append(delta)\n",
    "\n",
    "# Print for verification\n",
    "print(\"Adjacency matrix shape:\", A.shape)\n",
    "print(\"Laplacian matrix shape:\", L.shape)\n",
    "print(\"Max in-degree (delta):\", delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d92eab4e-3333-442e-838d-edbedfe634ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (20, 2, 500)\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "# Define x_noise before calling the function\n",
    "x_noise = np.zeros(n * m)\n",
    "x_noise[::2] = 1  # Noise on u-component (even indices: u1, v1, u2, v2, ...)\n",
    "\n",
    "def data_generate(n, m, transient, time, beta, mu, sigma, C, L, delta, h, gamma, x_noise):\n",
    "    def rulkov_map(x):\n",
    "        x = x.reshape(n, m).T\n",
    "        return np.asarray([\n",
    "            beta / (1 + x[0]**2) + x[1],\n",
    "            x[1] - mu * x[0] - sigma\n",
    "        ]).T.flatten()\n",
    "\n",
    "    def net_dynamics(x):\n",
    "        return rulkov_map(x) - (C / delta) * sparse.kron(L, h).dot(x) + x_noise * gamma * np.random.uniform(-1, 1, n * m)\n",
    "\n",
    "    # Initialize\n",
    "    x0 = np.random.uniform(0.0, 1.0, n * m)  # Flattened: [u1, v1, u2, v2, ...]\n",
    "    # Transient for isolated dynamics\n",
    "    for _ in range(transient):\n",
    "        x0 = rulkov_map(x0)\n",
    "    # Transient for coupled dynamics\n",
    "    for _ in range(transient):\n",
    "        x0 = net_dynamics(x0)\n",
    "    # Simulate\n",
    "    x = np.zeros((n * m, time))\n",
    "    x[:, 0] = x0\n",
    "    for t in range(time - 1):\n",
    "        x[:, t + 1] = net_dynamics(x[:, t])\n",
    "    return x.reshape(n, m, time)\n",
    "\n",
    "# Coupling matrix h (x-directional)\n",
    "h = np.eye(m)\n",
    "h[1, 1] = 0\n",
    "\n",
    "# Generate data\n",
    "x = data_generate(n, m, transient, time, beta, mu, sigma, C, L, delta, h, gamma, x_noise)\n",
    "storage[\"data\"].append(x)\n",
    "\n",
    "# Verify\n",
    "print(\"Data shape:\", x.shape)  # Should be (n, m, time) = (20, 2, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef58a7d9-ab6e-40f5-a8df-85b7a657201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model coefficients shape: (20, 2, 12)\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.linear_model import Lasso\n",
    "from pysindy.optimizers import BaseOptimizer\n",
    "\n",
    "# Custom optimizer to use sklearn's Lasso with PySINDy\n",
    "class LassoOptimizer(BaseOptimizer):\n",
    "    def __init__(self, alpha=0.0001, fit_intercept=False, max_iter=100000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lasso = Lasso(alpha=alpha, fit_intercept=fit_intercept, max_iter=max_iter)\n",
    "\n",
    "    def _reduce(self, x, y):\n",
    "        self.lasso.fit(x, y)\n",
    "        self.coef_ = self.lasso.coef_\n",
    "        self.ind_ = np.abs(self.coef_) > 1e-10  # Threshold for non-zero coefficients\n",
    "        return self\n",
    "\n",
    "def data_split(n, x):\n",
    "    X = x[:, :, :-1]  # x(t)\n",
    "    dx = x[:, :, 1:]  # x(t+1)\n",
    "    return X, dx\n",
    "\n",
    "def predicted_models(n, X, dx, time):\n",
    "    # Define custom library functions (SM Sec. VIII)\n",
    "    library_functions = [\n",
    "        lambda x: np.sin(x),\n",
    "        lambda x: np.cos(x),\n",
    "        lambda x: 1 / (1 - x),\n",
    "        lambda x: 1 / (1 - x**2),\n",
    "        lambda x: 1 / ((1 - x)**2),\n",
    "        lambda x: 1 / (1 + x),\n",
    "        lambda x: 1 / (1 + x**2),  # Critical for Rulkov\n",
    "        lambda x: 1 / ((1 + x)**2),\n",
    "        lambda x: 1 / x\n",
    "    ]\n",
    "    library_function_names = [\n",
    "        lambda x: f'sin({x})',\n",
    "        lambda x: f'cos({x})',\n",
    "        lambda x: f'1/(1-{x})',\n",
    "        lambda x: f'1/(1-{x}^2)',\n",
    "        lambda x: f'1/(1-{x})^2',\n",
    "        lambda x: f'1/(1+{x})',\n",
    "        lambda x: f'1/(1+{x}^2)',\n",
    "        lambda x: f'1/(1+{x})^2',\n",
    "        lambda x: f'1/{x}'\n",
    "    ]\n",
    "    # Combine custom and polynomial libraries\n",
    "    lib_custom = ps.CustomLibrary(library_functions=library_functions, function_names=library_function_names)\n",
    "    lib_poly = ps.PolynomialLibrary(degree=1, include_bias=True, include_interaction=False)\n",
    "    lib = ps.GeneralizedLibrary(\n",
    "        [lib_custom, lib_poly],\n",
    "        inputs_per_library=[[0, 0], [0, 1]]  # List to avoid warning\n",
    "    )\n",
    "\n",
    "    coeff = []\n",
    "    t = np.arange(time - 1)  # Time array for discrete-time fitting\n",
    "    for i in range(n):\n",
    "        model = ps.SINDy(\n",
    "            feature_library=lib,\n",
    "            optimizer=LassoOptimizer(alpha=0.0001, fit_intercept=False, max_iter=100000),\n",
    "            discrete_time=True\n",
    "        )\n",
    "        model.fit(X[i].T, t, x_dot=dx[i].T)  # Add time array t\n",
    "        coeff.append(model.coefficients())\n",
    "    return np.array(coeff)\n",
    "\n",
    "# Split data\n",
    "X, dx = data_split(n, x)\n",
    "# Fit models\n",
    "pred_models = predicted_models(n, X, dx, time)\n",
    "print(\"Model coefficients shape:\", pred_models.shape)  # Should be (n, m, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3be83fa2-4215-4993-99c0-4bdd1fca5c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted hub: 12, in-degree: 3.026245420104368\n",
      "Predicted low-degree node: 17, in-degree: 0.9140794903499686\n"
     ]
    }
   ],
   "source": [
    "def similarity(n, x, coeff, k_in):\n",
    "    # Ground-truth correlation (for reference, not used in classification)\n",
    "    corr_matrix_gt = np.corrcoef(x[:, 0, :], x[:, 0, :])[0:n, 0:n]\n",
    "    # Distance matrix between model coefficients\n",
    "    distance_matrix = pairwise_distances(coeff[:, 0, :], metric='seuclidean')\n",
    "    # Row-sum histogram\n",
    "    s = np.sum(distance_matrix, axis=1)\n",
    "    s_gt = np.sum(np.abs(corr_matrix_gt), axis=1)\n",
    "    hub_id = np.argmax(s)  # Hub: max distance (most different model)\n",
    "    ld_id = np.argmin(s)   # Low-degree: min distance\n",
    "    print(f\"Predicted hub: {hub_id}, in-degree: {k_in[hub_id]}\")\n",
    "    print(f\"Predicted low-degree node: {ld_id}, in-degree: {k_in[ld_id]}\")\n",
    "    return corr_matrix_gt, distance_matrix, s, s_gt, hub_id, ld_id\n",
    "\n",
    "# Classify\n",
    "corr_matrix_gt, distance_matrix, s, s_gt, hub_id, ld_id = similarity(n, x, pred_models, k_in)\n",
    "storage[\"dist_matrix\"].append(distance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4defc067-88f1-4605-b84c-0f5765367095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local dynamics coefficients shape: (2, 12)\n"
     ]
    }
   ],
   "source": [
    "def learn_local_dynamics(n, coeff, s):\n",
    "    # Select low-degree nodes (e.g., bottom 50% of s)\n",
    "    threshold = np.percentile(s, 50)\n",
    "    low_degree_indices = np.where(s <= threshold)[0]\n",
    "    # Average their coefficients\n",
    "    f_coeff = np.mean(coeff[low_degree_indices], axis=0)\n",
    "    return f_coeff\n",
    "\n",
    "# Learn f\n",
    "f_coeff = learn_local_dynamics(n, pred_models, s)\n",
    "print(\"Local dynamics coefficients shape:\", f_coeff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6432d67c-eabb-40e0-80f1-ecab237ee861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/artun/Documents/GitHub/network-reconstruction/.venv/lib/python3.13/site-packages/pysindy/feature_library/generalized_library.py:101: UserWarning: inputs_per_library should no longer be passed as a numpy array\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fx shape: (20, 2, 499)\n",
      "Y_hub shape: (2, 499)\n",
      "Y shape: (20, 2, 499)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pysindy as ps\n",
    "\n",
    "def local_dynamics_function(x, coeff, library):\n",
    "    \"\"\"\n",
    "    Compute F_x by evaluating the SINDy library at x and applying coefficients.\n",
    "    x: Input state, shape (m,) = (2,) for one node at one time.\n",
    "    coeff: SINDy coefficients, shape (m, num_features) = (2, 12).\n",
    "    library: SINDy GeneralizedLibrary object from predicted_models.\n",
    "    Returns: Predicted next state, shape (m,) = (2,).\n",
    "    \"\"\"\n",
    "    # Evaluate library functions at x\n",
    "    # Reshape x to (1, m) for SINDy\n",
    "    x_reshaped = x.reshape(1, -1)  # Shape (1, 2)\n",
    "    features = library.fit_transform(x_reshaped)  # Shape (1, num_features)\n",
    "    # Apply coefficients: features @ coeff.T\n",
    "    # features: (1, num_features), coeff.T: (num_features, m)\n",
    "    return np.dot(features, coeff.T).flatten()  # Shape (m,) = (2,)\n",
    "\n",
    "def coupling_effect(dx, hub_id, Fx):\n",
    "    \"\"\"\n",
    "    Compute coupling terms: Y_hub for hub node, Y for all nodes.\n",
    "    dx: Node derivatives, shape (n, m, time-1).\n",
    "    hub_id: Index of hub node.\n",
    "    Fx: Predicted local dynamics, shape (n, m, time-1).\n",
    "    Returns: Y_hub (m, time-1), Y (n, m, time-1).\n",
    "    \"\"\"\n",
    "    Y_hub = dx[hub_id, :, :] - Fx[hub_id, :, :]\n",
    "    Y = dx - Fx\n",
    "    return Y_hub, Y\n",
    "\n",
    "# Re-define the SINDy library (same as in predicted_models)\n",
    "library_functions = [\n",
    "    lambda x: np.sin(x),\n",
    "    lambda x: np.cos(x),\n",
    "    lambda x: 1 / (1 - x),\n",
    "    lambda x: 1 / (1 - x**2),\n",
    "    lambda x: 1 / ((1 - x)**2),\n",
    "    lambda x: 1 / (1 + x),\n",
    "    lambda x: 1 / (1 + x**2),\n",
    "    lambda x: 1 / ((1 + x)**2),\n",
    "    lambda x: 1 / x\n",
    "]\n",
    "library_function_names = [\n",
    "    lambda x: f'sin({x})',\n",
    "    lambda x: f'cos({x})',\n",
    "    lambda x: f'1/(1-{x})',\n",
    "    lambda x: f'1/(1-{x}^2)',\n",
    "    lambda x: f'1/(1-{x})^2',\n",
    "    lambda x: f'1/(1+{x})',\n",
    "    lambda x: f'1/(1+{x}^2)',\n",
    "    lambda x: f'1/(1+{x})^2',\n",
    "    lambda x: f'1/{x}'\n",
    "]\n",
    "lib_custom = ps.CustomLibrary(library_functions=library_functions, function_names=library_function_names)\n",
    "lib_poly = ps.PolynomialLibrary(degree=1, include_bias=True, include_interaction=False)\n",
    "lib = ps.GeneralizedLibrary([lib_custom, lib_poly], inputs_per_library=np.array([[0, 0], [0, 1]]))\n",
    "\n",
    "# Compute F_x for all nodes and times\n",
    "Fx = np.zeros((n, m, time - 1))\n",
    "for i in range(n):\n",
    "    for t in range(time - 1):\n",
    "        Fx[i, :, t] = local_dynamics_function(X[i, :, t], f_coeff, lib)\n",
    "\n",
    "# Compute coupling effects\n",
    "Y_hub, Y = coupling_effect(dx, hub_id, Fx)\n",
    "\n",
    "# Store results\n",
    "storage[\"Fx\"].append(Fx)\n",
    "storage[\"Y_hub\"].append(Y_hub)\n",
    "storage[\"Y\"].append(Y)\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Fx shape:\", Fx.shape)  # Should be (n, m, time-1) = (20, 2, 499)\n",
    "print(\"Y_hub shape:\", Y_hub.shape)  # Should be (m, time-1) = (2, 499)\n",
    "print(\"Y shape:\", Y.shape)  # Should be (n, m, time-1) = (20, 2, 499)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5a3d61e-44d4-427a-a7c8-6ab84302adb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (20, 2, 499)\n",
      "Y shape: (20, 2, 499)\n",
      "X_flat shape: (499, 40)\n",
      "Y_flat shape: (499, 40)\n",
      "t shape: (499,)\n",
      "L_pred raw shape: (40, 40)\n",
      "Extracted L_pred shape: (20, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from pysindy.optimizers import BaseOptimizer\n",
    "import numpy as np\n",
    "import pysindy as ps\n",
    "\n",
    "# Custom Lasso optimizer for PySINDy\n",
    "class LassoOptimizer(BaseOptimizer):\n",
    "    def __init__(self, alpha=0.0001, fit_intercept=False, max_iter=100000, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.lasso = Lasso(alpha=alpha, fit_intercept=fit_intercept, max_iter=max_iter)\n",
    "\n",
    "    def _reduce(self, x, y):\n",
    "        self.lasso.fit(x, y)\n",
    "        self.coef_ = self.lasso.coef_\n",
    "        self.ind_ = np.abs(self.coef_) > 1e-10  # Threshold for non-zero coefficients\n",
    "        return self\n",
    "\n",
    "def reconstruction(n, m, time, X, Y):\n",
    "    # Verify input shapes\n",
    "    print(\"X shape:\", X.shape)  # Expected: (n, m, time-1)\n",
    "    print(\"Y shape:\", Y.shape)  # Expected: (n, m, time-1)\n",
    "    \n",
    "    # Reshape for SINDy: (time-1, n*m)\n",
    "    X_flat = X.reshape(n * m, time - 1).T  # (time-1, n*m)\n",
    "    Y_flat = Y.reshape(n * m, time - 1).T  # (time-1, n*m)\n",
    "    print(\"X_flat shape:\", X_flat.shape)\n",
    "    print(\"Y_flat shape:\", Y_flat.shape)\n",
    "    \n",
    "    # Time array for discrete-time fitting\n",
    "    t = np.arange(time - 1)\n",
    "    print(\"t shape:\", t.shape)  # Expected: (time-1,)\n",
    "    \n",
    "    # Initialize SINDy with custom Lasso optimizer\n",
    "    model = ps.SINDy(\n",
    "        feature_library=ps.IdentityLibrary(),\n",
    "        optimizer=LassoOptimizer(alpha=0.001, fit_intercept=False),\n",
    "        discrete_time=True\n",
    "    )\n",
    "    \n",
    "    # Fit model with time array\n",
    "    model.fit(X_flat, t=t, x_dot=Y_flat)\n",
    "    \n",
    "    # Get coefficients (n*m x n*m)\n",
    "    L_pred = model.coefficients()\n",
    "    print(\"L_pred raw shape:\", L_pred.shape)\n",
    "    \n",
    "    # Extract n x n Laplacian (x-component coupling)\n",
    "    xxx = np.arange(0, n * m, m)\n",
    "    xx, yy = np.meshgrid(xxx, xxx)\n",
    "    L_pred = L_pred[xx, yy]\n",
    "    print(\"Extracted L_pred shape:\", L_pred.shape)  # Should be (n, n)\n",
    "    \n",
    "    return L_pred\n",
    "\n",
    "# Reconstruct Laplacian\n",
    "L_pred = reconstruction(n, m, time, X, Y)\n",
    "storage[\"predicted_laplacian\"].append(L_pred)\n",
    "\n",
    "# Save storage\n",
    "pickle.dump(storage, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea4c1d45-1fe4-4134-ae08-61f0c6253d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True L shape: (20, 20)\n",
      "Predicted L shape: (20, 20)\n",
      "RMSE: 0.35384628174298616\n"
     ]
    }
   ],
   "source": [
    "def rmse(L, L_pred, C, delta):\n",
    "    # Ensure shapes match\n",
    "    print(\"True L shape:\", L.shape)\n",
    "    print(\"Predicted L shape:\", L_pred.shape)\n",
    "    error = np.sqrt(np.mean((L - (C / delta) * L_pred)**2))\n",
    "    return error\n",
    "\n",
    "# Compute error\n",
    "error = rmse(L, L_pred, C, delta)\n",
    "print(\"RMSE:\", error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
